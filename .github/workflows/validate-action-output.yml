name: Validate Action Output
# Reusable workflow for testing the GitHub Action with OpenTelemetry collector validation
# This workflow can be used to test different trigger events (push, workflow_run, etc.)
#
# To update test data for any workflow using this reusable workflow:
# 1. Run the calling workflow (by pushing changes, creating PR, or triggering workflow_run)
# 2. After workflow completion, download the artifact from the GitHub Actions run page
# 3. Extract the artifact and copy traces-actual.json and metrics-actual.json to the appropriate .github/test-data/ directory
# 4. Rename them to traces-expected.json and metrics-expected.json (removing the normalized values)
# 5. Check the differences old vs new expected files to ensure the changes are valid
# 6. Commit and push the updated test data files

on:
  workflow_call:
    inputs:
      test-data-directory:
        description:
          'Directory name under .github/test-data/ for expected test data'
        required: true
        type: string
      artifact-name:
        description: 'Name for the collector logs artifact'
        required: false
        type: string
        default: 'collector-logs'
      retention-days:
        description: 'Number of days to retain the artifact'
        required: false
        type: number
        default: 5

jobs:
  test-action:
    name: GitHub Actions Test
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        id: checkout
        uses: actions/checkout@v5

      # Create directory with proper permissions
      - name: Create log directory with proper permissions
        run: |
          mkdir -p collector-logs
          chmod 777 collector-logs

      # Start OpenTelemetry Collector manually because GitHub Actions service container does not read custom config files before checkout
      - name: Start OpenTelemetry Collector
        # port 13133 is used for health checks, 4318 for OTLP HTTP endpoint
        run: |
          echo "Starting OpenTelemetry Collector with custom configuration..."
          docker run -d \
            --name otel-collector \
            -p 13133:13133 \
            -p 4318:4318 \
            -v ${{ github.workspace }}/.github/configs/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml:ro \
            -v ${{ github.workspace }}/collector-logs:/collector-logs \
            --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:13133/ || exit 1" \
            --health-interval 10s \
            --health-timeout 5s \
            --health-retries 3 \
            otel/opentelemetry-collector-contrib:0.131.0

      # Wait for collector to be healthy and ready
      - name: Wait for collector to be ready
        run: |
          echo "Waiting for OpenTelemetry Collector to be ready..."
          timeout 60s bash -c 'until docker ps | grep -q "healthy.*otel-collector"; do sleep 2; echo "Waiting for collector health check..."; done'
          echo "Collector is ready"

      # Execute the GitHub Action which sends telemetry data to the collector
      - name: Execute Action
        uses: ./
        env:
          OTEL_SERVICE_NAME: github-actions-opentelemetry
          # Point to local collector service (not external endpoint)
          OTEL_EXPORTER_OTLP_ENDPOINT: http://localhost:4318
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Wait for file system operations
        run: |
          echo "Waiting for file writes to complete..."
          sleep 5

      - name: Setup normalization filters
        run: |
          # replace dynamic values with fixed values
          cat > /tmp/normalize-traces.jq << 'EOF'
          (.resourceSpans[]?.scopeSpans[]?.spans[]?.traceId) = "00000000000000000000000000000000" |
          (.resourceSpans[]?.scopeSpans[]?.spans[]?.spanId) = "0000000000000000" |
          (.resourceSpans[]?.scopeSpans[]?.spans[]?.parentSpanId) = "0000000000000000" |
          (.resourceSpans[]?.scopeSpans[]?.spans[]?.startTimeUnixNano) = "0" |
          (.resourceSpans[]?.scopeSpans[]?.spans[]?.endTimeUnixNano) = "0" |
          .resourceSpans[]?.scopeSpans[]?.spans[]?.attributes[]? |= (
            if .key == "run_id" then .value = {"intValue": "0"}
            elif .key == "job.id" then .value = {"intValue": "0"}
            elif .key == "url" then .value = {"stringValue": "https://example.com/actions/runs/0"}
            elif .key == "runner.name" then .value = {"stringValue": "GitHub Actions 0"}
            else .
            end
          ) | 

          # sort spans by name because they can be in different order
          (.resourceSpans[]?.scopeSpans[]?.spans) |= sort_by(.name) |

          # sort all entries in objects to ensure stable output
          walk(if type == "object" then to_entries | sort_by(.key) | from_entries else . end)
          EOF

          # replace dynamic values with fixed values
          cat > /tmp/normalize-metrics.jq << 'EOF'
          (.resourceMetrics[]?.scopeMetrics[]?.metrics[]?.gauge?.dataPoints[]?.startTimeUnixNano) = "0" |
          (.resourceMetrics[]?.scopeMetrics[]?.metrics[]?.gauge?.dataPoints[]?.timeUnixNano) = "0" |
          (.resourceMetrics[]?.scopeMetrics[]?.metrics[]?.gauge?.dataPoints[]?.asDouble) = 0 |

          # sort metrics array by name to ensure stable output
          (.resourceMetrics[]?.scopeMetrics[]?.metrics) |= sort_by(.name) |

          # sort gauge data points by attributes for many data points
          (.resourceMetrics[]?.scopeMetrics[]?.metrics[]?.gauge?.dataPoints) |= sort_by(
            [.attributes[]? | select(.key == "job.name") | .value.stringValue][0] // "",
            [.attributes[]? | select(.key == "workflow.name") | .value.stringValue][0] // ""
          ) |           

          # sort all entries in objects to ensure stable output
          walk(if type == "object" then to_entries | sort_by(.key) | from_entries else . end)
          EOF

      - name: Generate normalized files
        run: |
          jq -f /tmp/normalize-traces.jq collector-logs/traces-exported-original.json > collector-logs/traces-actual.json
          jq -f /tmp/normalize-metrics.jq collector-logs/metrics-exported-original.json > collector-logs/metrics-actual.json

      # Upload collector logs as GitHub artifact for debugging
      - name: Upload collector logs as artifact
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: ${{ inputs.artifact-name }}
          path: collector-logs/
          retention-days: ${{ inputs.retention-days }}

      - name: Validate Traces
        run: |
          diff -u collector-logs/traces-actual.json .github/test-data/${{ inputs.test-data-directory }}/traces-expected.json
          if [ $? -ne 0 ]; then
            echo "✗ Trace structure differs from expected format"
            exit 1
          fi

      - name: Validate Metrics
        run: |
          diff -u collector-logs/metrics-actual.json .github/test-data/${{ inputs.test-data-directory }}/metrics-expected.json
          if [ $? -ne 0 ]; then
            echo "✗ Metrics structure differs from expected format"
            exit 1
          fi

      # Show final collector status and logs for debugging
      - name: Show collector debug info
        if: always()
        run: |
          echo "=== Final collector status ==="
          docker ps -a | grep otel-collector || echo "Collector container not found"

          echo "=== Complete collector logs ==="
          docker logs otel-collector || echo "Could not retrieve collector logs"

          echo "=== All generated files ==="
          find collector-logs -type f -exec echo "File: {}" \; -exec head -10 {} \; -exec echo "---" \; 2>/dev/null || echo "No files found"
